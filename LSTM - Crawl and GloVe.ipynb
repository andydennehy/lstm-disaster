{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Disaster Tweets with Long-Short Term Memory Networks\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" alt=\"Drawing\" style=\"width: 250px;\"/>"},{"metadata":{},"cell_type":"markdown","source":"Welcome to my Disaster Tweet LSTM notebook, where we'll try to train a LSTM (or *Long-Short Term Memory*) neural network on the disaster tweets to predict whether they are real or not. This notebook produced a public score of 0.81, which is pretty good for starters.\n\nYou can notice that I am using _various data sources_ for this notebook. That's because I used the Gensim Word Embedding dataset featured here: https://www.kaggle.com/iezepov/gensim-embeddings-dataset. These are basically pre-trained embeddings (that is, word->vector of real numbers mappings) based on a huge number of documents by the state-of-the-art people at Google, Stanford, Facebook and the such. We will exploit these instead of training embeddings ourselves to save time (and also, they work better).\n\nLet's start by importing all the required libraries and setting variables:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Math and linear algebra\nimport numpy as np\n\n# Data manipulation\nimport pandas as pd\n\n# Plots\nimport seaborn as sns\nimport matplotlib.style as style\nimport matplotlib.pyplot as plt\n\n# Keras: high-level neural network API \nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate, Reshape\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Dropout\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten\nfrom keras.preprocessing import text, sequence\n\n# Sk-learn\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Gensim: NLP library. We will use this to load the word embedding vectors\nfrom gensim.models import KeyedVectors\n\nimport re\n\n# Keras: deep learning API\nimport keras\nprint(keras.__version__)\nimport tensorflow\nprint(tensorflow.__version__)\n\n# Embedding files\nEMBEDDING_FILES = [\n    '../input/gensim-embeddings-dataset/crawl-300d-2M.gensim',\n    '../input/gensim-embeddings-dataset/glove.840B.300d.gensim',\n    '../input/gensim-embeddings-dataset/GoogleNews-vectors-negative300.gensim'\n]\n\n# \nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 20\nMAX_LEN = 220\nMAX_WORDS = 27\nTEXT_COLUMN = 'text'\nTARGET_COLUMN = 'target'\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We imported gensim to deal with the Embeddings dataset, as we'll see later. The rest are the usual Kaggle competition imports :-).\n\nThen we set up a number of \"constants\" (actually variables) which we are going to use in the rest of the notebook. First we set up the embedding datasets we are going to use: in this case, I used fasttext's crawl dataset trained on Common Crawl and Wikipedia (https://fasttext.cc/docs/en/crawl-vectors.html) and GloVe, trained by Stanford on the same sources (https://nlp.stanford.edu/projects/glove/).\n\nThen I set the batch size, the number of LSTM units, number of dense hidden units, epochs, max sequence length (220 characters since it's Twitter), and some chars to remove from the tweets.\n\nNow let's load the data and see what we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Read Data\")\ntrain_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have an id column (with missing numbers, which are the test id's), keyword, location, the tweet itself, and target - that is, whether it's an actual disaster or not.\n\nNow let's do some analytics.\n\n# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are\", train_df.shape[0], \"tweets in the training dataset.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are 7613 tweets, and that 30%+ of the locations are missing, as well as ~5% of keywords:"},{"metadata":{"trusted":true},"cell_type":"code","source":"style.use(\"ggplot\")\nx = train_df.isna().sum()*100.0/train_df.shape[0]\nax = sns.barplot(x.index, x, palette=\"Blues_d\")\nax.set_ylabel(\"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v = train_df[\"target\"].value_counts()\nsns.barplot(v.index, v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes=plt.subplots(1,2,figsize=(10,5), sharey='row')\nfig.suptitle(\"Tweet length in characters\")\nfor i in range(2):\n    sns.distplot(train_df[train_df[\"target\"] == i][\"text\"].str.len(), ax=axes[i], color=sns.color_palette()[i])\n    axes[i].set_xlabel(\"Real = \"+str(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes=plt.subplots(1,2,figsize=(10,5), sharey='row')\nfig.suptitle(\"Tweet length in words\")\nfor i in range(2):\n    sns.distplot(train_df[train_df[\"target\"] == i][\"text\"].str.split().map(lambda x: len(x)), ax=axes[i], color=sns.color_palette()[i])\n    axes[i].set_xlabel(\"Real = \"+str(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes=plt.subplots(1,2,figsize=(9,5))\nfig.suptitle(\"NaNs by variable\")\nv = train_df[\"keyword\"].isnull().groupby(train_df[\"target\"]).sum().astype(int)\nsns.barplot(v.index, v, ax=axes[0])\naxes[0].set_ylabel(\"# of NaNs\")\naxes[0].set_xlabel(\"\")\naxes[0].set_title(\"Keyword\")\n\nv = train_df[\"location\"].isnull().groupby(train_df[\"target\"]).sum().astype(int)\nsns.barplot(v.index, v, ax=axes[1])\naxes[1].set_ylabel(\"# of NaNs\")\naxes[1].set_xlabel(\"\")\naxes[1].set_title(\"Location\")\n\nfig.tight_layout(rect=[0, 0.03, 1, 0.90])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common words\nfrom collections import Counter\nfig, axes = plt.subplots(1, 2, figsize=(14,5))\nfig.suptitle(\"Most common words\")\nfor i in range(2):\n    word_list = [x for sublist in list(train_df[train_df[\"target\"] == i][\"text\"].map(lambda x: x.split()).values) for x in sublist]\n    top = sorted(Counter(word_list).items(), key=lambda x:x[1], reverse=True)[:10]\n    x, y = zip(*top)\n    axes[i].bar(x, y)\n    axes[i].set_xlabel(\"Real = \"+str(i))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"x_train = train_df[TEXT_COLUMN].astype(str)\ny_train = train_df[TARGET_COLUMN].values\nx_test = test_df[TEXT_COLUMN].astype(str)\n\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE, lower=False)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model compilation, training and prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_predictions = []\nweights = []\n\ndef build_matrix(word_index, path):\n    embedding_index = KeyedVectors.load(path, mmap='r')\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        for candidate in [word, word.lower()]:\n            if candidate in embedding_index:\n                embedding_matrix[i] = embedding_index[candidate]\n                break\n    return embedding_matrix\n\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n\ndef build_model(embedding_matrix):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, dropout=0.2))(x)\n    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, dropout=0.2))(x)\n    #x = Dropout(0.4)(x)\n    #res = Reshape((-1, x_train.shape[1], 100))(x)\n    #conv2 = Conv2D(100, (3,3), padding='same',activation=\"relu\")(x)\n    #pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n    #flat = Flatten()(pool2)\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x)\n    ])\n    dense = Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)\n    dense = Dense(DENSE_HIDDEN_UNITS, activation='relu')(dense)\n    result = Dense(1, activation='sigmoid')(dense)\n    model = Model(inputs=words, outputs=result)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Start Modeling\")\nmodel = build_model(embedding_matrix)\nmodel.summary()\nmodel.fit(\n    x_train,\n    y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_split=0.2,\n    verbose=1\n)\nprint(\"Modeling Complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(x_test)\nsubmission = pd.DataFrame.from_dict({\n    'id': test_df.id,\n    TARGET_COLUMN: np.round(predictions).astype(int).reshape(x_test.shape[0])\n})\nsubmission.to_csv('lstm_submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
